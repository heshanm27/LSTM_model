{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5509f87-fc5d-4c3a-b4a8-5c57e754e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow  opencv-python mediapipe sklearn matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "273e4766-3355-49a9-afc6-25560bebbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "#load train model \n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3134b099-3f55-4972-ade0-b99e35a2c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = np.array(['අ', 'ආ', 'ඇ','ඈ','ඉ','ඊ','උ','ඌ','එ','ඒ','ඔ','ඕ','ක්','ග්','ජ්','ට්','ද්','ණ්','ත්','ඩ්','න්','ප්','බ්','ම්','ය්','ර්','ල්','ව්','ස්','හ්'])\n",
    "actions = np.array(['අ', 'ආ', 'ඇ','ඈ','ඉ'])\t\t\t\t\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "# Each One Video  going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "# Folder start\n",
    "start_folder = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50898217-abc0-444d-bf53-86e73608779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            48896     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203525 (795.02 KB)\n",
      "Trainable params: 203525 (795.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('action.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2586913-455b-45bf-8dfe-6b3927640791",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(16,117,245),(16,117,245)]\n",
    "font = ImageFont.truetype(\"D:\\Research\\sinahlaFont.ttf\", 15)\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "         # Convert  Numpy array and switch  from BGR to RGB\n",
    "        image = cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(image)\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        #dynamic_text = actions[num];\n",
    "        dynamic_text = actions[num];\n",
    "           # Calculate the text position\n",
    "        text_x = 0,60+n   # Adjust the starting position of the text\n",
    "        text_y = 75 + num * 40\n",
    "        draw.text((text_x, text_y), dynamic_text, font=font, fill=(255, 255, 255))  \n",
    "    \n",
    "                        \n",
    "        # Convert back to Numpy array and switch back from RGB to BGR\n",
    "        converted_image = np.asarray(pil_image)\n",
    "        output_frame = cv2.cvtColor(converted_image, cv2.COLOR_RGB2BGR)\n",
    "   \n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932210c2-a8e9-48c5-9217-165ae02c0a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(prob_viz(\u001b[43mres\u001b[49m,actions,image,colors))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(prob_viz(res,actions,image,colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cb6d50-dfc6-4b01-b73e-ef53b692ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beca240a-1bca-4867-a745-c40f80f41f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c08ecd-88a2-4a08-880c-8b15d3f438ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5c45d0-7317-476f-85aa-4a7029778188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abecca6f-0b87-4e5c-9b03-c8fb3aff6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 407ms/step\n",
      "ඉ\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m         sentence \u001b[38;5;241m=\u001b[39m sentence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Viz probabilities\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mprob_viz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(image, (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m40\u001b[39m), (\u001b[38;5;241m245\u001b[39m, \u001b[38;5;241m117\u001b[39m, \u001b[38;5;241m16\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m      \u001b[38;5;66;03m# Convert  Numpy array and switch  from BGR to RGB\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mprob_viz\u001b[1;34m(res, actions, input_frame, colors)\u001b[0m\n\u001b[0;32m     14\u001b[0m text_x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)   \u001b[38;5;66;03m# Adjust the starting position of the text\u001b[39;00m\n\u001b[0;32m     15\u001b[0m text_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m \u001b[38;5;241m+\u001b[39m num \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert back to Numpy array and switch back from RGB to BGR\u001b[39;00m\n\u001b[0;32m     20\u001b[0m converted_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pil_image)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageDraw.py:556\u001b[0m, in \u001b[0;36mImageDraw.text\u001b[1;34m(self, xy, text, fill, font, anchor, spacing, align, direction, features, language, stroke_width, stroke_fill, embedded_color, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m     draw_text(ink, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# Only draw normal text\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[43mdraw_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mink\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageDraw.py:496\u001b[0m, in \u001b[0;36mImageDraw.text.<locals>.draw_text\u001b[1;34m(ink, stroke_width, stroke_offset)\u001b[0m\n\u001b[0;32m    494\u001b[0m start \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 496\u001b[0m     coord\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    497\u001b[0m     start\u001b[38;5;241m.\u001b[39mappend(math\u001b[38;5;241m.\u001b[39mmodf(xy[i])[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "font = ImageFont.truetype(\"D:\\Research\\sinahlaFont.ttf\", 15)\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "             # Convert  Numpy array and switch  from BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(image)\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        dynamic_text = ' '.join(sentence);\n",
    "        draw.text((10, 10),dynamic_text, font=font)\n",
    "    \n",
    "                        \n",
    "        # Convert back to Numpy array and switch back from RGB to BGR\n",
    "        converted_image = np.asarray(pil_image)\n",
    "        cv2image = cv2.cvtColor(converted_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', cv2image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af94ab32-8639-4b7b-8406-5ac0b982c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de750203-a9c3-42fe-9bc3-8a590859d8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
